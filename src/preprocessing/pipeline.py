# src/preprocessing/pipeline.py

import logging
from pathlib import Path
from typing import Dict, Tuple, Union, Optional
import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler
import json

from .feature_engineering import FeatureEngineer
from src.utils.config import Config

class Pipeline:
    """
    The Pipeline class handles:
    - Data validation and cleaning
    - Feature engineering
    - Normalization/scaling of non-target features and the target (close) separately
    - For training: splitting data and preparing sequences (X, Y).
    - For prediction: returning only the last sequence_length rows scaled for input.
    """

    def __init__(
        self,
        config: Optional[Config] = None,
        sequence_length: int = 60,
        prediction_length: int = 1,
        test_size: float = 0.2,
        validation_size: float = 0.2,
        feature_scaler_type: str = 'standard',
        target_scaler_type: str = 'robust',
    ):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

        self.config = config

        if config and isinstance(config, Config):
            model_config = config.get_model_config()
            data_config = config.get_data_config()
            preprocessing_config = config.get_preprocessing_config()

            self.sequence_length = model_config.get('sequence_length', sequence_length)
            self.prediction_length = model_config.get('prediction_length', prediction_length)

            self.test_size = data_config.get('test_split', test_size)
            self.validation_size = data_config.get('validation_split', validation_size)

            scaling_config = preprocessing_config.get('scaling', {})
            self.feature_scaler_type = scaling_config.get('feature_scaler_type', feature_scaler_type)
            self.target_scaler_type = scaling_config.get('target_scaler_type', target_scaler_type)
        else:
            self.sequence_length = sequence_length
            self.prediction_length = prediction_length
            self.test_size = test_size
            self.validation_size = validation_size
            self.feature_scaler_type = feature_scaler_type
            self.target_scaler_type = target_scaler_type

        if self.test_size + self.validation_size >= 1.0:
            raise ValueError("Sum of test_size and validation_size must be less than 1.")

        self.scaler = self._initialize_scaler(self.feature_scaler_type)
        self.target_scaler = self._initialize_scaler(self.target_scaler_type)

        self.numeric_features = []
        self.scalers_fitted = False

        fe_config = None
        if self.config:
            preprocessing_config = self.config.get_preprocessing_config()
            if preprocessing_config and 'feature_engineering' in preprocessing_config:
                fe_config = preprocessing_config['feature_engineering']

        self.feature_engineer = FeatureEngineer(config=fe_config)

        self.logger.info(f"Pipeline initialized with sequence_length={self.sequence_length}, "
                         f"prediction_length={self.prediction_length}, test_size={self.test_size}, "
                         f"validation_size={self.validation_size}, "
                         f"feature_scaler_type={self.feature_scaler_type}, target_scaler_type={self.target_scaler_type}")

    @staticmethod
    def _initialize_scaler(scaler_type: str):
        if scaler_type == 'minmax':
            return MinMaxScaler()
        elif scaler_type == 'robust':
            return RobustScaler()
        elif scaler_type == 'standard':
            return StandardScaler()
        else:
            raise ValueError(f"Unsupported scaler type: {scaler_type}")

    def _update_numeric_features(self, df: pd.DataFrame):
        # Exclude 'close' from numeric_features to ensure it's only scaled by target_scaler
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.numeric_features = [col for col in numeric_cols if col != 'close']
        self.logger.info(f"Numeric features identified (excluding close): {self.numeric_features}")

    def validate_data(self, df: pd.DataFrame):
        if df.empty:
            raise ValueError("Input DataFrame is empty.")

        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

        if df[required_columns].isnull().values.any():
            raise ValueError("Required columns contain missing values.")

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("Applying feature engineering...")
        df_features = self.feature_engineer.add_technical_features(df)
        return df_features

    def fit_normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            raise ValueError("Input DataFrame is empty.")
        df_scaled = df.copy()
        self._update_numeric_features(df_scaled)

        if self.config:
            scaling_config = self.config.get_preprocessing_config().get('scaling', {})
            if scaling_config.get('price_transform') == 'log1p':
                self.logger.info("Applying log1p transformation to price columns (open, high, low, close).")
                price_cols = ['open', 'high', 'low', 'close']
                df_scaled[price_cols] = np.log1p(df_scaled[price_cols])

            if scaling_config.get('volume_transform') == 'log1p':
                self.logger.info("Applying log1p transformation to volume.")
                df_scaled['volume'] = np.log1p(df_scaled['volume'])

        # Scale numeric features (excluding 'close')
        if self.numeric_features:
            self.logger.info("Fitting scaler on numeric features.")
            df_scaled[self.numeric_features] = self.scaler.fit_transform(df_scaled[self.numeric_features])

        # Scale target close separately
        if 'close' in df_scaled.columns:
            self.logger.info("Fitting scaler on target ('close').")
            df_scaled['close'] = self.target_scaler.fit_transform(df_scaled[['close']]).flatten()

        self.scalers_fitted = True
        return df_scaled

    def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            raise ValueError("Input DataFrame is empty.")
        if not self.scalers_fitted:
            raise ValueError("Scalers have not been fitted. Call fit_normalize_features first.")

        df_scaled = df.copy()

        if self.config:
            scaling_config = self.config.get_preprocessing_config().get('scaling', {})
            if scaling_config.get('price_transform') == 'log1p':
                self.logger.info("Applying log1p transform to price columns.")
                price_cols = ['open', 'high', 'low', 'close']
                existing_price_cols = [c for c in price_cols if c in df_scaled.columns]
                if existing_price_cols:
                    df_scaled[existing_price_cols] = np.log1p(df_scaled[existing_price_cols])

            if scaling_config.get('volume_transform') == 'log1p' and 'volume' in df_scaled.columns:
                self.logger.info("Applying log1p transform to volume.")
                df_scaled['volume'] = np.log1p(df_scaled['volume'])

        # Temporarily remove close for feature scaling
        close_col = None
        if 'close' in df_scaled.columns:
            close_col = df_scaled['close'].copy()
            df_scaled.drop(columns=['close'], inplace=True, errors='ignore')

        # Ensure only known numeric_features remain
        all_current_numeric = df_scaled.select_dtypes(include=[np.number]).columns.tolist()
        current_set = set(all_current_numeric)
        expected_set = set(self.numeric_features)

        extra_features = current_set - expected_set
        if extra_features:
            self.logger.warning(f"Dropping extra numeric features not seen at fit time: {extra_features}")
            df_scaled.drop(columns=list(extra_features), inplace=True, errors='ignore')

        missing_features = expected_set - set(df_scaled.columns)
        for mf in missing_features:
            self.logger.warning(f"Missing feature {mf} at prediction time. Filling with zeros.")
            df_scaled[mf] = 0.0

        df_scaled = df_scaled.reindex(columns=self.numeric_features, fill_value=0.0)
        df_scaled[self.numeric_features] = self.scaler.transform(df_scaled[self.numeric_features])

        # Re-add close using target_scaler if it existed
        if close_col is not None:
            close_scaled = self.target_scaler.transform(close_col.to_frame()).flatten()
            df_scaled['close'] = close_scaled

        return df_scaled

    def run(self, df: pd.DataFrame, save_dir: Optional[str] = None, prediction_mode: bool = False) -> Dict[str, np.ndarray]:
        if df.empty:
            raise ValueError("Input DataFrame is empty.")
        self.validate_data(df)
        df_features = self.create_features(df)

        # Remove bfill - only forward fill to prevent data leakage
        if df_features.isnull().values.any():
            initial_rows = len(df_features)
            self.logger.warning("Missing values detected, applying forward fill only (no bfill).")
            df_features.ffill(inplace=True)
            df_features.dropna(inplace=True)
            dropped = initial_rows - len(df_features)
            if dropped > 0:
                self.logger.info(f"Dropped {dropped} rows with NaN values after ffill.")

        if df_features.empty:
            raise ValueError("No data left after handling missing values.")

        self._update_numeric_features(df_features)

        if prediction_mode:
            if self.scalers_fitted:
                df_normalized = self.normalize_features(df_features)
            else:
                df_normalized = self.fit_normalize_features(df_features)

            if len(df_normalized) < self.sequence_length + self.prediction_length:
                raise ValueError(f"Not enough data: need {self.sequence_length + self.prediction_length}, got {len(df_normalized)}")

            # Get last sequence for input
            last_seq = df_normalized.iloc[-self.sequence_length:]
            # Remove 'close' column - it will be in features during training but not needed for X
            last_seq_features = last_seq.drop(columns=['close'], errors='ignore')
            X = np.expand_dims(last_seq_features.values, axis=0)
            
            # Store last known price for inverse transform
            last_price = df_features['close'].iloc[-1]
            return {'X': X, 'last_price': last_price}

        train_df, val_df, test_df = self.split_data(df_features)

        train_norm = self.fit_normalize_features(train_df)
        val_norm = self.normalize_features(val_df)
        test_norm = self.normalize_features(test_df)

        X_train, Y_train, train_prices = self.prepare_sequences(train_norm, df_features.loc[train_norm.index])
        X_val, Y_val, val_prices = self.prepare_sequences(val_norm, df_features.loc[val_norm.index])
        X_test, Y_test, test_prices = self.prepare_sequences(test_norm, df_features.loc[test_norm.index])

        result = {
            'X_train': X_train,
            'y_train': Y_train,
            'X_val': X_val,
            'y_val': Y_val,
            'X_test': X_test,
            'y_test': Y_test,
            'train_last_prices': train_prices,
            'val_last_prices': val_prices,
            'test_last_prices': test_prices,
            'numeric_features': self.numeric_features
        }

        if save_dir:
            self.save_processed_data(result, save_dir)
            scaler_dir = Path(save_dir) / "scalers"
            scaler_dir.mkdir(parents=True, exist_ok=True)
            self.save_scaler(
                scaler_dir / "feature_scaler.joblib",
                scaler_dir / "target_scaler.joblib"
            )

        return result

    def prepare_sequences(self, df_normalized: pd.DataFrame, df_original: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Create sequences for multi-horizon return forecasting.
        
        Args:
            df_normalized: Normalized dataframe with all features including 'close'
            df_original: Original dataframe (before normalization) to get actual prices
        
        Returns:
            X: shape (N, sequence_length, features) - input sequences (without 'close')
            Y: shape (N, forecast_horizon) - target log returns for next 5 days
            last_prices: shape (N,) - last known price for each sequence (for inverse transform)
        """
        if df_normalized.empty or df_original.empty:
            raise ValueError("Empty DataFrame, cannot prepare sequences.")
        
        forecast_horizon = self.prediction_length  # Should be 5 from config
        min_length = self.sequence_length + forecast_horizon
        
        if len(df_normalized) < min_length:
            raise ValueError(f"Not enough data: need {min_length}, got {len(df_normalized)}")
        
        X, Y, last_prices = [], [], []
        max_start_idx = len(df_original) - self.sequence_length - forecast_horizon + 1
        
        for i in range(max_start_idx):
            # Input sequence: all features EXCEPT close (close is target, not feature)
            seq = df_normalized.iloc[i:i + self.sequence_length]
            seq_features = seq.drop(columns=['close'], errors='ignore')
            X.append(seq_features.values)
            
            # Get original prices for return calculation
            target_start = i + self.sequence_length
            target_end = target_start + forecast_horizon
            
            # Calculate log returns for next 5 days
            prices = df_original.iloc[target_start - 1:target_end]['close'].values
            if len(prices) != forecast_horizon + 1:
                continue  # Skip if not enough future data
            
            # Log returns: log(price_t / price_{t-1})
            log_returns = np.log(prices[1:] / prices[0])
            Y.append(log_returns)
            
            # Store last known price (for converting predictions back to prices)
            last_prices.append(prices[0])
        
        X = np.array(X)
        Y = np.array(Y)
        last_prices = np.array(last_prices)
        
        self.logger.info(f"Created {len(X)} sequences: X={X.shape}, Y={Y.shape} (returns)")
        return X, Y, last_prices

    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        min_size = max(self.sequence_length + self.prediction_length, 200)
        if len(df) < min_size * 3:
            raise ValueError("Not enough data for splitting into train/val/test.")

        train_size = 1 - self.test_size - self.validation_size
        n = len(df)
        buffer_size = 100
        train_end = int(n * train_size)
        val_end = train_end + int(n * self.validation_size)

        train = df.iloc[:train_end + buffer_size].copy()
        val = df.iloc[max(0, train_end - buffer_size):val_end + buffer_size].copy()
        test = df.iloc[max(0, val_end - buffer_size):].copy()

        self.logger.info(f"Data split: Train={len(train)}, Val={len(val)}, Test={len(test)}")
        return train, val, test

    def save_processed_data(self, processed_data: Dict[str, np.ndarray], save_dir: str):
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        for key, value in processed_data.items():
            if isinstance(value, np.ndarray):
                np.save(save_path / f"{key}.npy", value)
            elif key == 'numeric_features' and isinstance(value, list):
                with open(save_path / f"{key}.json", 'w') as f:
                    json.dump(value, f)
            else:
                self.logger.warning(f"Skipping saving {key}, unrecognized type {type(value)}")

    def save_scaler(self, scaler_path: Union[str, Path], target_scaler_path: Union[str, Path]):
        scaler_path = Path(scaler_path)
        target_scaler_path = Path(target_scaler_path)
        joblib.dump(self.scaler, scaler_path)
        joblib.dump(self.target_scaler, target_scaler_path)
        self.logger.info(f"Saved scalers to {scaler_path} and {target_scaler_path}")
        self.scalers_fitted = True

    def load_scaler(self, scaler_path: Union[str, Path], target_scaler_path: Union[str, Path]):
        scaler_path = Path(scaler_path)
        target_scaler_path = Path(target_scaler_path)

        if not scaler_path.exists() or not target_scaler_path.exists():
            raise FileNotFoundError("Scaler files not found.")

        self.scaler = joblib.load(scaler_path)
        self.target_scaler = joblib.load(target_scaler_path)
        self.scalers_fitted = True
        self.logger.info(f"Loaded scalers from {scaler_path} and {target_scaler_path}")

    def inverse_transform_predictions(self, returns: np.ndarray, last_prices: np.ndarray) -> np.ndarray:
        """
        Convert predicted log returns back to actual prices.
        
        Args:
            returns: shape (N, horizon) or (horizon,) - predicted log returns
            last_prices: shape (N,) or scalar - last known price(s) before forecast
        
        Returns:
            Predicted prices, shape (N, horizon) or (horizon,)
        """
        single_prediction = False
        if returns.ndim == 1:
            returns = returns.reshape(1, -1)
            single_prediction = True
        
        if np.isscalar(last_prices):
            last_prices = np.array([last_prices])
        
        # Convert log returns to prices: price_t = price_0 * exp(sum of returns up to t)
        prices_list = []
        for i, (ret_seq, last_price) in enumerate(zip(returns, last_prices)):
            prices = [last_price]
            for r in ret_seq:
                next_price = prices[-1] * np.exp(r)
                prices.append(next_price)
            prices_list.append(prices[1:])  # Exclude the initial price
        
        result = np.array(prices_list)
        return result[0] if single_prediction else result

    def inverse_transform_actuals(self, y: np.ndarray, last_prices: np.ndarray) -> np.ndarray:
        """
        Convert actual log returns back to prices for evaluation.
        
        Args:
            y: shape (N, horizon) - actual log returns
            last_prices: shape (N,) - last known prices
        
        Returns:
            Actual prices, shape (N, horizon)
        """
        return self.inverse_transform_predictions(y, last_prices)

    def save(self, path: Union[str, Path]):
        path = Path(path)
        joblib.dump({
            'scaler': self.scaler,
            'target_scaler': self.target_scaler,
            'numeric_features': self.numeric_features
        }, path)
        self.logger.info(f"Pipeline saved to {path}")

    @classmethod
    def load(cls, path: Union[str, Path]):
        data = joblib.load(path)
        pipeline = cls()
        pipeline.scaler = data['scaler']
        pipeline.target_scaler = data['target_scaler']
        pipeline.numeric_features = data['numeric_features']
        pipeline.scalers_fitted = True
        logging.info(f"Pipeline loaded from {path}")
        return pipeline
